{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSPGc2Jvpp7q",
        "outputId": "a7e39c44-68ee-4cc1-f141-254c75e63549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jSZic_u5-mq",
        "outputId": "794153f3-b12c-42f4-dba6-72160bd8455d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Crawling\n",
            "/content/drive/MyDrive/Crawling\n",
            "BeautifulSoup.ipynb\n"
          ]
        }
      ],
      "source": [
        "# 마운트 -> 구글 드라이브를 불러 작업할 디렉토리(현재 파일의 위치)를 지정해준다\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Crawling\n",
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEgMdg6xr98X",
        "outputId": "7f725d45-0eba-400f-c6b0-fec530b4e8e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<h3 class=\"Box-title\">Search cheat sheet</h3>\n",
            "<h3 class=\"Box-title\">Search cheat sheet</h3>\n",
            "Search cheat sheet\n",
            "Search cheat sheet\n",
            "[<h3 class=\"Box-title\">Search cheat sheet</h3>, <h3>\n",
            "    25,043 repository results\n",
            "\n",
            "\n",
            "</h3>]\n",
            "[<a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":1,\"click_id\":529502,\"result\":{\"id\":529502,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk1Mjk1MDI=\",\"model_name\":\"Repository\",\"url\":\"https://github.com/scrapy/scrapy\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"1b5491ef94832c63cf7f48882b5fbb9ab7a2f0c7ab35aed2d82b7c56b54cbc3f\" href=\"/scrapy/scrapy\">scrapy/scrapy</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":2,\"click_id\":54357610,\"result\":{\"id\":54357610,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk1NDM1NzYxMA==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/dotnetcore/DotnetSpider\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"32c15047b39be1a0fd99a9d7072f22f601e4a0d6bc8b69bccce9e3831c551f47\" href=\"/dotnetcore/DotnetSpider\">dotnetcore/DotnetSpider</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":3,\"click_id\":28904322,\"result\":{\"id\":28904322,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnkyODkwNDMyMg==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/waditu/tushare\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"4b6566a89ff68a4e5bf2546ff4b99f844a415dbb3416401cddce89af6ee33813\" href=\"/waditu/tushare\">waditu/tushare</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":4,\"click_id\":10072612,\"result\":{\"id\":10072612,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnkxMDA3MjYxMg==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/qinxuye/cola\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"dd5a3349959a29d922c0b6e8fb7d9fdfb7cf256cc64a9f4f0bf20091b7fafa0a\" href=\"/qinxuye/cola\">qinxuye/cola</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":5,\"click_id\":92926018,\"result\":{\"id\":92926018,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk5MjkyNjAxOA==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/gaojiuli/gain\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"fb48ae60809f054686920eb4d69c18f63a94025a49b5f2f73653deb62dc69bcd\" href=\"/gaojiuli/gain\">gaojiuli/gain</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":6,\"click_id\":6945670,\"result\":{\"id\":6945670,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk2OTQ1Njcw\",\"model_name\":\"Repository\",\"url\":\"https://github.com/crawljax/crawljax\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"e7b7c7e4a9ea4458234c89efec52fe1971c1a607682b46e23d4efbe34139d6dc\" href=\"/crawljax/crawljax\">crawljax/crawljax</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":7,\"click_id\":100910675,\"result\":{\"id\":100910675,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnkxMDA5MTA2NzU=\",\"model_name\":\"Repository\",\"url\":\"https://github.com/qiyaTech/javaCrawling\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"248342c21423bae175feba678b4111c967ccc7935ab336bef1e4037de0f40b20\" href=\"/qiyaTech/javaCrawling\">qiyaTech/java<em>Crawling</em></a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":8,\"click_id\":6553098,\"result\":{\"id\":6553098,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk2NTUzMDk4\",\"model_name\":\"Repository\",\"url\":\"https://github.com/mjhea0/Scrapy-Samples\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"64543ba30eca3a543d210b5215772f901fb994766eb0eda6e1cd7560d768bbdb\" href=\"/mjhea0/Scrapy-Samples\">mjhea0/Scrapy-Samples</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":9,\"click_id\":23038334,\"result\":{\"id\":23038334,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnkyMzAzODMzNA==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/crawl/crawl\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"8827ed9549c28bb44549649edbe135abbdc7a4185df6d9adfb49e59a042041ba\" href=\"/crawl/crawl\">crawl/crawl</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":10,\"click_id\":20805787,\"result\":{\"id\":20805787,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnkyMDgwNTc4Nw==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/chensoul/scrapy-zhihu-github\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"e5e135b1e028d6e592e47f350ceddf5683958eb0db340c4245f71d86e56d25e9\" href=\"/chensoul/scrapy-zhihu-github\">chensoul/scrapy-zhihu-github</a>]\n",
            "scrapy/scrapy\n",
            "dotnetcore/DotnetSpider\n",
            "waditu/tushare\n",
            "qinxuye/cola\n",
            "gaojiuli/gain\n",
            "crawljax/crawljax\n",
            "qiyaTech/javaCrawling\n",
            "mjhea0/Scrapy-Samples\n",
            "crawl/crawl\n",
            "chensoul/scrapy-zhihu-github\n"
          ]
        }
      ],
      "source": [
        "# 기본 설명용 코드\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://github.com/search?p=1&q=crawling&type=Repositories'\n",
        "\n",
        "# 웹페이지에 데이터 요청을 보냄\n",
        "response = requests.get(url)\n",
        "\n",
        "# html코드중 올바른 반응이왔을때 (404에러같은게 뜨거나 서버에서 블록한게 아닌)\n",
        "if response.status_code == 200:\n",
        "    # html 코드 그대로 받음\n",
        "    html = response.text\n",
        "    # xml이나 json으로 구성된 페이지가 아니므로 html파서로 beautifulsoup라이브러리용 데이터 변환\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # 가장 위의 태그 하나만 받는 코드들\n",
        "    test=soup.h3\n",
        "    print(test)\n",
        "    test=soup.find('h3')\n",
        "    print(test)\n",
        "\n",
        "    # html태그제외한 텍스트만 추출하는 방법\n",
        "    test=soup.find('h3').get_text()\n",
        "    print(test)\n",
        "    test=soup.find('h3').string\n",
        "    print(test)\n",
        "\n",
        "    # 조건에 맞는 모든 태그들 받는 코드들\n",
        "    test=soup.find_all('h3')\n",
        "    print(test)\n",
        "    attrs = {'class': 'v-align-middle'}\n",
        "    test = soup.find_all('a', attrs=attrs)\n",
        "    print(test)\n",
        "    # all로 받은 태그 리스트에서 for문으로 텍스트만 추출하려면\n",
        "    for code in test:\n",
        "        print(code.get_text())\n",
        "\n",
        "else : \n",
        "    print('연결되지않음')\n",
        "    print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQW8vQkbp-JM",
        "outputId": "ad213272-cf51-44f1-ca05-102ea69f86da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "crawling\n",
            "['scrapy', 'dotnetcore', 'waditu', 'qinxuye', 'gaojiuli', 'crawljax', 'qiyaTech', 'mjhea0', 'crawl', 'chensoul']\n",
            "['scrapy', 'DotnetSpider', 'tushare', 'cola', 'gain', 'crawljax', 'javaCrawling', 'Scrapy-Samples', 'crawl', 'scrapy-zhihu-github']\n",
            "['Scrapy, a fast high-level web crawling & scraping framework for Python.', 'DotnetSpider, a .NET standard web crawling library. It is lightweight, efficient and fast high-level web crawling & s…', 'TuShare is a utility for crawling historical data of China stocks', 'A high-level distributed crawling framework.', 'Web crawling framework based on asyncio.', 'Crawljax: Crawling Dynamic (JavaScript-based) Web Applications', '\"奇伢爬虫\"是基于sprint boot 、 WebMagic 实现 微信公众号文章、新闻、csdn、info等网站文章爬取，可以动态设置文章爬取规则、清洗规则，基本实现了爬取大部分网站的文章。', 'Scrapy examples crawling Craigslist', 'Dungeon Crawl: Stone Soup official repository', 'scrapy examples for crawling zhihu and github']\n",
            "['43.4k', '3.3k', '11.5k', '1.4k', '2k', '459', '294', '192', '1.7k', '217']\n",
            "['Python', 'C#', 'Python', 'Python', 'Python', 'JavaScript', 'Java', 'Python', 'C++', 'Python']\n",
            "['scrapy', 'dotnetcore', 'waditu', 'qinxuye', 'gaojiuli', 'crawljax', 'qiyaTech', 'mjhea0', 'crawl', 'chensoul', 'Tuhinshubhra', 's0md3v', 'hjkl01', 'symfony', 'soskek', 'crush-one', 'elixir-crawly', 'alex-miller-0', 'bplawler', 'NikolaiT']\n",
            "['scrapy', 'DotnetSpider', 'tushare', 'cola', 'gain', 'crawljax', 'javaCrawling', 'Scrapy-Samples', 'crawl', 'scrapy-zhihu-github', 'RED_HAWK', 'uro', 'pornhub', 'panther', 'bookcorpus', 'cracking4crawling', 'crawly', 'Tor_Crawler', 'crawler', 'Crawling-Infrastructure']\n",
            "['Scrapy, a fast high-level web crawling & scraping framework for Python.', 'DotnetSpider, a .NET standard web crawling library. It is lightweight, efficient and fast high-level web crawling & s…', 'TuShare is a utility for crawling historical data of China stocks', 'A high-level distributed crawling framework.', 'Web crawling framework based on asyncio.', 'Crawljax: Crawling Dynamic (JavaScript-based) Web Applications', '\"奇伢爬虫\"是基于sprint boot 、 WebMagic 实现 微信公众号文章、新闻、csdn、info等网站文章爬取，可以动态设置文章爬取规则、清洗规则，基本实现了爬取大部分网站的文章。', 'Scrapy examples crawling Craigslist', 'Dungeon Crawl: Stone Soup official repository', 'scrapy examples for crawling zhihu and github', 'All in one tool for Information Gathering, Vulnerability Scanning and Crawling. A must have tool for all penetration …', 'declutters url lists for crawling/pentesting', 'crawl webm and mp4', 'A browser testing and web crawling library for PHP and Symfony', 'Crawl BookCorpus', '一些爬虫相关的签名、验证码破解，目前已涉及：小红书。', 'Crawly, a high-level web crawling & scraping framework for Elixir.', 'Web crawling with IP rotation via Tor', 'Scala DSL for web crawling', 'Distributed crawling infrastructure running on top of severless computation, cloud storage (such as S3) and sophistic…']\n",
            "['43.4k', '3.3k', '11.5k', '1.4k', '2k', '459', '294', '192', '1.7k', '217', '2k', '471', '521', '2.5k', '536', '81', '556', '196', '144', '321']\n",
            "['Python', 'C#', 'Python', 'Python', 'Python', 'JavaScript', 'Java', 'Python', 'C++', 'Python', 'PHP', 'Python', 'Python', 'PHP', 'Python', 'Python', 'Elixir', 'Python', 'Scala', 'TypeScript']\n",
            "['scrapy', 'dotnetcore', 'waditu', 'qinxuye', 'gaojiuli', 'crawljax', 'qiyaTech', 'mjhea0', 'crawl', 'chensoul', 'Tuhinshubhra', 's0md3v', 'hjkl01', 'symfony', 'soskek', 'crush-one', 'elixir-crawly', 'alex-miller-0', 'bplawler', 'NikolaiT', 'geziyor', 'meibenjin', 'ZhangBohan', 'ChrisRx', 'duckduckgo', 'kootenpv', 'zorlan', 'afunTW', 'codelucas', 'mion']\n",
            "['scrapy', 'DotnetSpider', 'tushare', 'cola', 'gain', 'crawljax', 'javaCrawling', 'Scrapy-Samples', 'crawl', 'scrapy-zhihu-github', 'RED_HAWK', 'uro', 'pornhub', 'panther', 'bookcorpus', 'cracking4crawling', 'crawly', 'Tor_Crawler', 'crawler', 'Crawling-Infrastructure', 'geziyor', 'GoogleSearchCrawler', 'fun_crawler', 'dungeonfs', 'duckduckcrawl', 'sky', 'skycaiji', 'Python-Crawling-Tutorial', 'newspaper', 'harvestman']\n",
            "['Scrapy, a fast high-level web crawling & scraping framework for Python.', 'DotnetSpider, a .NET standard web crawling library. It is lightweight, efficient and fast high-level web crawling & s…', 'TuShare is a utility for crawling historical data of China stocks', 'A high-level distributed crawling framework.', 'Web crawling framework based on asyncio.', 'Crawljax: Crawling Dynamic (JavaScript-based) Web Applications', '\"奇伢爬虫\"是基于sprint boot 、 WebMagic 实现 微信公众号文章、新闻、csdn、info等网站文章爬取，可以动态设置文章爬取规则、清洗规则，基本实现了爬取大部分网站的文章。', 'Scrapy examples crawling Craigslist', 'Dungeon Crawl: Stone Soup official repository', 'scrapy examples for crawling zhihu and github', 'All in one tool for Information Gathering, Vulnerability Scanning and Crawling. A must have tool for all penetration …', 'declutters url lists for crawling/pentesting', 'crawl webm and mp4', 'A browser testing and web crawling library for PHP and Symfony', 'Crawl BookCorpus', '一些爬虫相关的签名、验证码破解，目前已涉及：小红书。', 'Crawly, a high-level web crawling & scraping framework for Elixir.', 'Web crawling with IP rotation via Tor', 'Scala DSL for web crawling', 'Distributed crawling infrastructure running on top of severless computation, cloud storage (such as S3) and sophistic…', 'Geziyor, blazing fast web crawling & scraping framework for Go. Supports JS rendering.', 'a tool for crawl Google search results', 'Crawl some picture for fun', 'A FUSE filesystem and dungeon crawling adventure game engine', 'Distributed crawling prototype for DuckDuckGO', '🌅 next generation web crawling using machine intelligence', '蓝天采集器是一款免费的数据采集发布爬虫软件，采用php+mysql开发，可部署在云服务器，几乎能采集所有类型的网页，无缝对接各类CMS建站程序，免登录实时发布数据，全自动无需人工干预！是网页大数据采集软件中完全跨平台的云端爬虫系统', 'Python crawling tutorial', 'News, full-text, and article metadata extraction in Python 3. Advanced docs:', 'Quick and dirty web crawling.']\n",
            "['43.4k', '3.3k', '11.5k', '1.4k', '2k', '459', '294', '192', '1.7k', '217', '2k', '471', '521', '2.5k', '536', '81', '556', '196', '144', '321', '1.7k', '350', '166', '990', '142', '298', '1.5k', '61', '11.8k', '276']\n",
            "['Python', 'C#', 'Python', 'Python', 'Python', 'JavaScript', 'Java', 'Python', 'C++', 'Python', 'PHP', 'Python', 'Python', 'PHP', 'Python', 'Python', 'Elixir', 'Python', 'Scala', 'TypeScript', 'Go', 'Python', 'Python', 'Go', 'Python', 'Python', 'PHP', 'Jupyter Notebook', 'Python', 'Ruby']\n",
            "['scrapy', 'dotnetcore', 'waditu', 'qinxuye', 'gaojiuli', 'crawljax', 'qiyaTech', 'mjhea0', 'crawl', 'chensoul', 'Tuhinshubhra', 's0md3v', 'hjkl01', 'symfony', 'soskek', 'crush-one', 'elixir-crawly', 'alex-miller-0', 'bplawler', 'NikolaiT', 'geziyor', 'meibenjin', 'ZhangBohan', 'ChrisRx', 'duckduckgo', 'kootenpv', 'zorlan', 'afunTW', 'codelucas', 'mion', 'commoncrawl', 'clemfromspace', 'lgraubner', 'riywo', 'yatima1460', 'RoyalIcing', 'github', 'REMitchell', 'vanangamudi', 'apache']\n",
            "['scrapy', 'DotnetSpider', 'tushare', 'cola', 'gain', 'crawljax', 'javaCrawling', 'Scrapy-Samples', 'crawl', 'scrapy-zhihu-github', 'RED_HAWK', 'uro', 'pornhub', 'panther', 'bookcorpus', 'cracking4crawling', 'crawly', 'Tor_Crawler', 'crawler', 'Crawling-Infrastructure', 'geziyor', 'GoogleSearchCrawler', 'fun_crawler', 'dungeonfs', 'duckduckcrawl', 'sky', 'skycaiji', 'Python-Crawling-Tutorial', 'newspaper', 'harvestman', 'commoncrawl', 'scrapy-selenium', 'sitemap-generator-cli', 'law.e-gov.go.jp', 'Drill', 'Lantern', 'lightcrawler', 'python-crawling', 'newspaper-crawler-scripts', 'nutch']\n",
            "['Scrapy, a fast high-level web crawling & scraping framework for Python.', 'DotnetSpider, a .NET standard web crawling library. It is lightweight, efficient and fast high-level web crawling & s…', 'TuShare is a utility for crawling historical data of China stocks', 'A high-level distributed crawling framework.', 'Web crawling framework based on asyncio.', 'Crawljax: Crawling Dynamic (JavaScript-based) Web Applications', '\"奇伢爬虫\"是基于sprint boot 、 WebMagic 实现 微信公众号文章、新闻、csdn、info等网站文章爬取，可以动态设置文章爬取规则、清洗规则，基本实现了爬取大部分网站的文章。', 'Scrapy examples crawling Craigslist', 'Dungeon Crawl: Stone Soup official repository', 'scrapy examples for crawling zhihu and github', 'All in one tool for Information Gathering, Vulnerability Scanning and Crawling. A must have tool for all penetration …', 'declutters url lists for crawling/pentesting', 'crawl webm and mp4', 'A browser testing and web crawling library for PHP and Symfony', 'Crawl BookCorpus', '一些爬虫相关的签名、验证码破解，目前已涉及：小红书。', 'Crawly, a high-level web crawling & scraping framework for Elixir.', 'Web crawling with IP rotation via Tor', 'Scala DSL for web crawling', 'Distributed crawling infrastructure running on top of severless computation, cloud storage (such as S3) and sophistic…', 'Geziyor, blazing fast web crawling & scraping framework for Go. Supports JS rendering.', 'a tool for crawl Google search results', 'Crawl some picture for fun', 'A FUSE filesystem and dungeon crawling adventure game engine', 'Distributed crawling prototype for DuckDuckGO', '🌅 next generation web crawling using machine intelligence', '蓝天采集器是一款免费的数据采集发布爬虫软件，采用php+mysql开发，可部署在云服务器，几乎能采集所有类型的网页，无缝对接各类CMS建站程序，免登录实时发布数据，全自动无需人工干预！是网页大数据采集软件中完全跨平台的云端爬虫系统', 'Python crawling tutorial', 'News, full-text, and article metadata extraction in Python 3. Advanced docs:', 'Quick and dirty web crawling.', 'Common Crawl support library to access 2008-2012 crawl archives (ARC files)', 'Scrapy middleware to handle javascript pages using selenium', 'Creates an XML-Sitemap by crawling a given site.', 'Crawling Japanese laws', 'Search files without indexing, but clever crawling', 'Mac app for website auditing and crawling', 'Crawl a website and run it through Google lighthouse', 'Code Repository for Web Crawling with Python', 'set of scripts for crawling newspaper websites.', 'Apache Nutch is an extensible and scalable web crawler']\n",
            "['43.4k', '3.3k', '11.5k', '1.4k', '2k', '459', '294', '192', '1.7k', '217', '2k', '471', '521', '2.5k', '536', '81', '556', '196', '144', '321', '1.7k', '350', '166', '990', '142', '298', '1.5k', '61', '11.8k', '276', '470', '719', '242', '164', '245', '187', '1.4k', '29', '24', '2.4k']\n",
            "['Python', 'C#', 'Python', 'Python', 'Python', 'JavaScript', 'Java', 'Python', 'C++', 'Python', 'PHP', 'Python', 'Python', 'PHP', 'Python', 'Python', 'Elixir', 'Python', 'Scala', 'TypeScript', 'Go', 'Python', 'Python', 'Go', 'Python', 'Python', 'PHP', 'Jupyter Notebook', 'Python', 'Ruby', 'C++', 'Python', 'JavaScript', 'Ruby', 'D', 'Swift', 'JavaScript', 'JavaScript', 'Python', 'Java']\n",
            "['scrapy', 'dotnetcore', 'waditu', 'qinxuye', 'gaojiuli', 'crawljax', 'qiyaTech', 'mjhea0', 'crawl', 'chensoul', 'Tuhinshubhra', 's0md3v', 'hjkl01', 'symfony', 'soskek', 'crush-one', 'elixir-crawly', 'alex-miller-0', 'bplawler', 'NikolaiT', 'geziyor', 'meibenjin', 'ZhangBohan', 'ChrisRx', 'duckduckgo', 'kootenpv', 'zorlan', 'afunTW', 'codelucas', 'mion', 'commoncrawl', 'clemfromspace', 'lgraubner', 'riywo', 'yatima1460', 'RoyalIcing', 'github', 'REMitchell', 'vanangamudi', 'apache', 'apache', 'YahooArchive', 'CrawlScript', 'zhuyingda', 'zx576', 'FLZ101', 'trivio', 'evanzd', 'TeamHG-Memex', 'wukenaihe']\n",
            "['scrapy', 'DotnetSpider', 'tushare', 'cola', 'gain', 'crawljax', 'javaCrawling', 'Scrapy-Samples', 'crawl', 'scrapy-zhihu-github', 'RED_HAWK', 'uro', 'pornhub', 'panther', 'bookcorpus', 'cracking4crawling', 'crawly', 'Tor_Crawler', 'crawler', 'Crawling-Infrastructure', 'geziyor', 'GoogleSearchCrawler', 'fun_crawler', 'dungeonfs', 'duckduckcrawl', 'sky', 'skycaiji', 'Python-Crawling-Tutorial', 'newspaper', 'harvestman', 'commoncrawl', 'scrapy-selenium', 'sitemap-generator-cli', 'law.e-gov.go.jp', 'Drill', 'Lantern', 'lightcrawler', 'python-crawling', 'newspaper-crawler-scripts', 'nutch', 'nutch', 'anthelion', 'WebCollector', 'webster', 'recruitment', 'dl_coursera', 'common_crawl_index', 'ICLR2021-OpenReviewData', 'arachnado', 'db-meta']\n",
            "['Scrapy, a fast high-level web crawling & scraping framework for Python.', 'DotnetSpider, a .NET standard web crawling library. It is lightweight, efficient and fast high-level web crawling & s…', 'TuShare is a utility for crawling historical data of China stocks', 'A high-level distributed crawling framework.', 'Web crawling framework based on asyncio.', 'Crawljax: Crawling Dynamic (JavaScript-based) Web Applications', '\"奇伢爬虫\"是基于sprint boot 、 WebMagic 实现 微信公众号文章、新闻、csdn、info等网站文章爬取，可以动态设置文章爬取规则、清洗规则，基本实现了爬取大部分网站的文章。', 'Scrapy examples crawling Craigslist', 'Dungeon Crawl: Stone Soup official repository', 'scrapy examples for crawling zhihu and github', 'All in one tool for Information Gathering, Vulnerability Scanning and Crawling. A must have tool for all penetration …', 'declutters url lists for crawling/pentesting', 'crawl webm and mp4', 'A browser testing and web crawling library for PHP and Symfony', 'Crawl BookCorpus', '一些爬虫相关的签名、验证码破解，目前已涉及：小红书。', 'Crawly, a high-level web crawling & scraping framework for Elixir.', 'Web crawling with IP rotation via Tor', 'Scala DSL for web crawling', 'Distributed crawling infrastructure running on top of severless computation, cloud storage (such as S3) and sophistic…', 'Geziyor, blazing fast web crawling & scraping framework for Go. Supports JS rendering.', 'a tool for crawl Google search results', 'Crawl some picture for fun', 'A FUSE filesystem and dungeon crawling adventure game engine', 'Distributed crawling prototype for DuckDuckGO', '🌅 next generation web crawling using machine intelligence', '蓝天采集器是一款免费的数据采集发布爬虫软件，采用php+mysql开发，可部署在云服务器，几乎能采集所有类型的网页，无缝对接各类CMS建站程序，免登录实时发布数据，全自动无需人工干预！是网页大数据采集软件中完全跨平台的云端爬虫系统', 'Python crawling tutorial', 'News, full-text, and article metadata extraction in Python 3. Advanced docs:', 'Quick and dirty web crawling.', 'Common Crawl support library to access 2008-2012 crawl archives (ARC files)', 'Scrapy middleware to handle javascript pages using selenium', 'Creates an XML-Sitemap by crawling a given site.', 'Crawling Japanese laws', 'Search files without indexing, but clever crawling', 'Mac app for website auditing and crawling', 'Crawl a website and run it through Google lighthouse', 'Code Repository for Web Crawling with Python', 'set of scripts for crawling newspaper websites.', 'Apache Nutch is an extensible and scalable web crawler', 'Apache Nutch is an extensible and scalable web crawler', 'Anthelion is a plugin for Apache Nutch to crawl semantic annotations within HTML pages.', 'WebCollector is an open source web crawler framework based on Java.It provides some simple interfaces for crawling th…', 'a reliable high-level web crawling & scraping framework for Node.js.', 'A project crawling online recruitment websites getting offer information', 'A simple, fast, and reliable Coursera crawling & downloading tool', 'Index URLs in Common Crawl', 'Crawl & visualize ICLR papers and reviews.', 'Web Crawling UI and HTTP API, based on Scrapy and Tornado', 'database meta crawl tool']\n",
            "['43.4k', '3.3k', '11.5k', '1.4k', '2k', '459', '294', '192', '1.7k', '217', '2k', '471', '521', '2.5k', '536', '81', '556', '196', '144', '321', '1.7k', '350', '166', '990', '142', '298', '1.5k', '61', '11.8k', '276', '470', '719', '242', '164', '245', '187', '1.4k', '29', '24', '2.4k', '2.4k', '2.9k', '2.9k', '385', '63', '128', '174', '408', '148', '78']\n",
            "['Python', 'C#', 'Python', 'Python', 'Python', 'JavaScript', 'Java', 'Python', 'C++', 'Python', 'PHP', 'Python', 'Python', 'PHP', 'Python', 'Python', 'Elixir', 'Python', 'Scala', 'TypeScript', 'Go', 'Python', 'Python', 'Go', 'Python', 'Python', 'PHP', 'Jupyter Notebook', 'Python', 'Ruby', 'C++', 'Python', 'JavaScript', 'Ruby', 'D', 'Swift', 'JavaScript', 'JavaScript', 'Python', 'Java', 'Java', 'Java', 'Java', 'JavaScript', 'JavaScript', 'Python', 'Python', 'Jupyter Notebook', 'Python', 'Java']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "users = []\n",
        "repositories = []\n",
        "descriptions = []\n",
        "stars = []\n",
        "languages = []\n",
        "\n",
        "# csv 제목 지정\n",
        "f = open(\"data.csv\", \"w\", encoding='utf-8')\n",
        "writer = csv.writer(f)\n",
        "data = [['user', 'repository', 'descripsion', 'star', 'language']]\n",
        "writer.writerows(data)\n",
        "\n",
        "\n",
        "# 데이터를 위 users, repositories, descriptions, stars, languages 리스트에 저장하는함수\n",
        "def get_data(url):\n",
        "  \n",
        "    # url = 'https://github.com/search?p=1&q=crawling&type=Repositories'\n",
        "    response = requests.get(url)\n",
        "    title_all = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        html = response.text\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        # 제목의 class를 이용해 제목받아옴.\n",
        "        attrs = {'class': 'v-align-middle'}\n",
        "        title = soup.find_all('a', attrs=attrs)\n",
        "        # print(title)\n",
        "        for i in title:\n",
        "            # 제목은 scrapy/scrapy 형식이고, 나중에 이를 쓸일이 있기에 title_all에 저장, 그리고 '/'로 스플릿하여 유저명과 리포지토리명 리스트에 저장\n",
        "            title_all.append(i.get_text())\n",
        "            titles = i.get_text().split('/')\n",
        "            users.append(titles[0])\n",
        "            repositories.append(titles[1])\n",
        "        print(users)\n",
        "        print(repositories)\n",
        "\n",
        "\n",
        "        # 설명부분의 class를 이용해 설명 받아옴. 텍스트를 뽑으면 \\n과 공백들이 많아서 strip()으로 앞뒤 불필요요소 제거함.(처음엔 공백과 \\n의 개수가 일정해서 인덱스로 가져왔었음.)\n",
        "        attrs = {'class': 'mb-1'}\n",
        "        description = soup.find_all('p', attrs=attrs)\n",
        "        # print(description)\n",
        "        for i in description:\n",
        "            # descriptions.append(i.get_text()[9:-7])\n",
        "            descriptions.append(i.get_text().strip())\n",
        "        print(descriptions)\n",
        "\n",
        "\n",
        "        # 별 갯수 가져오는데 class가 Link--muted인걸 가져와보니 class가 Link--muted f6인 '30 issues need help' 같은 문자도 같이 옴\n",
        "        # 그래서 class와 href 둘다 만족하는 조건으로 star가져옴. href가 /scrapy/scrapy/stargazers 로 '/' + 제목전체값 + '/stargazers'이기에 아까 title_all을 따로 저장했다 가져옴\n",
        "        # print(title_all)\n",
        "        for i in range(len(title_all)):\n",
        "            href = '/' + title_all[i] + '/stargazers'\n",
        "            # print(href)\n",
        "            attrs = {'class': 'Link--muted', 'href': href}\n",
        "            star = soup.find('a', attrs=attrs)\n",
        "            # stars.append(star.get_text()[18:-13])\n",
        "            stars.append(star.get_text().strip())\n",
        "        print(stars)\n",
        "\n",
        "        # 프로그래밍언어는 itemprop 값이 programmingLanguage인걸로 찾아옴.\n",
        "        attrs = {'itemprop': 'programmingLanguage'}\n",
        "        language = soup.find_all('span', attrs=attrs)\n",
        "        # print(language)\n",
        "        for i in language:\n",
        "            languages.append(i.get_text())\n",
        "        print(languages)\n",
        "\n",
        "    else:\n",
        "        print('연결되지않음')\n",
        "        print(response.status_code)\n",
        "\n",
        "\n",
        "# 키워드입력\n",
        "keyword = input()\n",
        "\n",
        "# 5페이지까지 키워드를 넣어서 검색하여 users, repositories, descriptions, stars, languages 리스트에 저장\n",
        "for i in range(1, 6):\n",
        "    url = 'https://github.com/search?p='+str(i)+'&q='+keyword+'&type=Repositories'\n",
        "    # 위에 정의한 리스트에 요소들 추가하는 함수(1~5페이지까지 돌면서 append함)\n",
        "    get_data(url)\n",
        "    # 빠른접근시 문제되어 10초간 여유 둠. 최적시간을 알면 줄일 수 있을듯.\n",
        "    time.sleep(10)\n",
        "\n",
        "# csv에 위 설정한 제목에 맞게끔 줄별로 입력\n",
        "for i in range(len(repositories)):\n",
        "    row = [[users[i], repositories[i], descriptions[i], stars[i], languages[i]]]\n",
        "    writer.writerows(row)\n",
        "\n",
        "# 다 입력하고 파일 닫음\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BeautifulSoup.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
