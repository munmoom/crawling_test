{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSPGc2Jvpp7q",
        "outputId": "a7e39c44-68ee-4cc1-f141-254c75e63549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jSZic_u5-mq",
        "outputId": "794153f3-b12c-42f4-dba6-72160bd8455d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Crawling\n",
            "/content/drive/MyDrive/Crawling\n",
            "BeautifulSoup.ipynb\n"
          ]
        }
      ],
      "source": [
        "# ë§ˆìš´íŠ¸ -> êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ ë¶ˆëŸ¬ ì‘ì—…í•  ë””ë ‰í† ë¦¬(í˜„ì¬ íŒŒì¼ì˜ ìœ„ì¹˜)ë¥¼ ì§€ì •í•´ì¤€ë‹¤\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Crawling\n",
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEgMdg6xr98X",
        "outputId": "7f725d45-0eba-400f-c6b0-fec530b4e8e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<h3 class=\"Box-title\">Search cheat sheet</h3>\n",
            "<h3 class=\"Box-title\">Search cheat sheet</h3>\n",
            "Search cheat sheet\n",
            "Search cheat sheet\n",
            "[<h3 class=\"Box-title\">Search cheat sheet</h3>, <h3>\n",
            "    25,043 repository results\n",
            "\n",
            "\n",
            "</h3>]\n",
            "[<a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":1,\"click_id\":529502,\"result\":{\"id\":529502,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk1Mjk1MDI=\",\"model_name\":\"Repository\",\"url\":\"https://github.com/scrapy/scrapy\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"1b5491ef94832c63cf7f48882b5fbb9ab7a2f0c7ab35aed2d82b7c56b54cbc3f\" href=\"/scrapy/scrapy\">scrapy/scrapy</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":2,\"click_id\":54357610,\"result\":{\"id\":54357610,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk1NDM1NzYxMA==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/dotnetcore/DotnetSpider\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"32c15047b39be1a0fd99a9d7072f22f601e4a0d6bc8b69bccce9e3831c551f47\" href=\"/dotnetcore/DotnetSpider\">dotnetcore/DotnetSpider</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":3,\"click_id\":28904322,\"result\":{\"id\":28904322,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnkyODkwNDMyMg==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/waditu/tushare\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"4b6566a89ff68a4e5bf2546ff4b99f844a415dbb3416401cddce89af6ee33813\" href=\"/waditu/tushare\">waditu/tushare</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":4,\"click_id\":10072612,\"result\":{\"id\":10072612,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnkxMDA3MjYxMg==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/qinxuye/cola\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"dd5a3349959a29d922c0b6e8fb7d9fdfb7cf256cc64a9f4f0bf20091b7fafa0a\" href=\"/qinxuye/cola\">qinxuye/cola</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":5,\"click_id\":92926018,\"result\":{\"id\":92926018,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk5MjkyNjAxOA==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/gaojiuli/gain\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"fb48ae60809f054686920eb4d69c18f63a94025a49b5f2f73653deb62dc69bcd\" href=\"/gaojiuli/gain\">gaojiuli/gain</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":6,\"click_id\":6945670,\"result\":{\"id\":6945670,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk2OTQ1Njcw\",\"model_name\":\"Repository\",\"url\":\"https://github.com/crawljax/crawljax\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"e7b7c7e4a9ea4458234c89efec52fe1971c1a607682b46e23d4efbe34139d6dc\" href=\"/crawljax/crawljax\">crawljax/crawljax</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":7,\"click_id\":100910675,\"result\":{\"id\":100910675,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnkxMDA5MTA2NzU=\",\"model_name\":\"Repository\",\"url\":\"https://github.com/qiyaTech/javaCrawling\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"248342c21423bae175feba678b4111c967ccc7935ab336bef1e4037de0f40b20\" href=\"/qiyaTech/javaCrawling\">qiyaTech/java<em>Crawling</em></a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":8,\"click_id\":6553098,\"result\":{\"id\":6553098,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnk2NTUzMDk4\",\"model_name\":\"Repository\",\"url\":\"https://github.com/mjhea0/Scrapy-Samples\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"64543ba30eca3a543d210b5215772f901fb994766eb0eda6e1cd7560d768bbdb\" href=\"/mjhea0/Scrapy-Samples\">mjhea0/Scrapy-Samples</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":9,\"click_id\":23038334,\"result\":{\"id\":23038334,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnkyMzAzODMzNA==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/crawl/crawl\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"8827ed9549c28bb44549649edbe135abbdc7a4185df6d9adfb49e59a042041ba\" href=\"/crawl/crawl\">crawl/crawl</a>, <a class=\"v-align-middle\" data-hydro-click='{\"event_type\":\"search_result.click\",\"payload\":{\"page_number\":1,\"per_page\":10,\"query\":\"crawling\",\"result_position\":10,\"click_id\":20805787,\"result\":{\"id\":20805787,\"global_relay_id\":\"MDEwOlJlcG9zaXRvcnkyMDgwNTc4Nw==\",\"model_name\":\"Repository\",\"url\":\"https://github.com/chensoul/scrapy-zhihu-github\"},\"originating_url\":\"https://github.com/search?p=1&amp;q=crawling&amp;type=Repositories\",\"user_id\":null}}' data-hydro-click-hmac=\"e5e135b1e028d6e592e47f350ceddf5683958eb0db340c4245f71d86e56d25e9\" href=\"/chensoul/scrapy-zhihu-github\">chensoul/scrapy-zhihu-github</a>]\n",
            "scrapy/scrapy\n",
            "dotnetcore/DotnetSpider\n",
            "waditu/tushare\n",
            "qinxuye/cola\n",
            "gaojiuli/gain\n",
            "crawljax/crawljax\n",
            "qiyaTech/javaCrawling\n",
            "mjhea0/Scrapy-Samples\n",
            "crawl/crawl\n",
            "chensoul/scrapy-zhihu-github\n"
          ]
        }
      ],
      "source": [
        "# ê¸°ë³¸ ì„¤ëª…ìš© ì½”ë“œ\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://github.com/search?p=1&q=crawling&type=Repositories'\n",
        "\n",
        "# ì›¹í˜ì´ì§€ì— ë°ì´í„° ìš”ì²­ì„ ë³´ëƒ„\n",
        "response = requests.get(url)\n",
        "\n",
        "# htmlì½”ë“œì¤‘ ì˜¬ë°”ë¥¸ ë°˜ì‘ì´ì™”ì„ë•Œ (404ì—ëŸ¬ê°™ì€ê²Œ ëœ¨ê±°ë‚˜ ì„œë²„ì—ì„œ ë¸”ë¡í•œê²Œ ì•„ë‹Œ)\n",
        "if response.status_code == 200:\n",
        "    # html ì½”ë“œ ê·¸ëŒ€ë¡œ ë°›ìŒ\n",
        "    html = response.text\n",
        "    # xmlì´ë‚˜ jsonìœ¼ë¡œ êµ¬ì„±ëœ í˜ì´ì§€ê°€ ì•„ë‹ˆë¯€ë¡œ htmlíŒŒì„œë¡œ beautifulsoupë¼ì´ë¸ŒëŸ¬ë¦¬ìš© ë°ì´í„° ë³€í™˜\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # ê°€ì¥ ìœ„ì˜ íƒœê·¸ í•˜ë‚˜ë§Œ ë°›ëŠ” ì½”ë“œë“¤\n",
        "    test=soup.h3\n",
        "    print(test)\n",
        "    test=soup.find('h3')\n",
        "    print(test)\n",
        "\n",
        "    # htmlíƒœê·¸ì œì™¸í•œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ëŠ” ë°©ë²•\n",
        "    test=soup.find('h3').get_text()\n",
        "    print(test)\n",
        "    test=soup.find('h3').string\n",
        "    print(test)\n",
        "\n",
        "    # ì¡°ê±´ì— ë§ëŠ” ëª¨ë“  íƒœê·¸ë“¤ ë°›ëŠ” ì½”ë“œë“¤\n",
        "    test=soup.find_all('h3')\n",
        "    print(test)\n",
        "    attrs = {'class': 'v-align-middle'}\n",
        "    test = soup.find_all('a', attrs=attrs)\n",
        "    print(test)\n",
        "    # allë¡œ ë°›ì€ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ forë¬¸ìœ¼ë¡œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ë ¤ë©´\n",
        "    for code in test:\n",
        "        print(code.get_text())\n",
        "\n",
        "else : \n",
        "    print('ì—°ê²°ë˜ì§€ì•ŠìŒ')\n",
        "    print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQW8vQkbp-JM",
        "outputId": "ad213272-cf51-44f1-ca05-102ea69f86da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "crawling\n",
            "['scrapy', 'dotnetcore', 'waditu', 'qinxuye', 'gaojiuli', 'crawljax', 'qiyaTech', 'mjhea0', 'crawl', 'chensoul']\n",
            "['scrapy', 'DotnetSpider', 'tushare', 'cola', 'gain', 'crawljax', 'javaCrawling', 'Scrapy-Samples', 'crawl', 'scrapy-zhihu-github']\n",
            "['Scrapy, a fast high-level web crawling & scraping framework for Python.', 'DotnetSpider, a .NET standard web crawling library. It is lightweight, efficient and fast high-level web crawling & sâ€¦', 'TuShare is a utility for crawling historical data of China stocks', 'A high-level distributed crawling framework.', 'Web crawling framework based on asyncio.', 'Crawljax: Crawling Dynamic (JavaScript-based) Web Applications', '\"å¥‡ä¼¢çˆ¬è™«\"æ˜¯åŸºäºsprint boot ã€ WebMagic å®ç° å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ã€æ–°é—»ã€csdnã€infoç­‰ç½‘ç«™æ–‡ç« çˆ¬å–ï¼Œå¯ä»¥åŠ¨æ€è®¾ç½®æ–‡ç« çˆ¬å–è§„åˆ™ã€æ¸…æ´—è§„åˆ™ï¼ŒåŸºæœ¬å®ç°äº†çˆ¬å–å¤§éƒ¨åˆ†ç½‘ç«™çš„æ–‡ç« ã€‚', 'Scrapy examples crawling Craigslist', 'Dungeon Crawl: Stone Soup official repository', 'scrapy examples for crawling zhihu and github']\n",
            "['43.4k', '3.3k', '11.5k', '1.4k', '2k', '459', '294', '192', '1.7k', '217']\n",
            "['Python', 'C#', 'Python', 'Python', 'Python', 'JavaScript', 'Java', 'Python', 'C++', 'Python']\n",
            "['scrapy', 'dotnetcore', 'waditu', 'qinxuye', 'gaojiuli', 'crawljax', 'qiyaTech', 'mjhea0', 'crawl', 'chensoul', 'Tuhinshubhra', 's0md3v', 'hjkl01', 'symfony', 'soskek', 'crush-one', 'elixir-crawly', 'alex-miller-0', 'bplawler', 'NikolaiT']\n",
            "['scrapy', 'DotnetSpider', 'tushare', 'cola', 'gain', 'crawljax', 'javaCrawling', 'Scrapy-Samples', 'crawl', 'scrapy-zhihu-github', 'RED_HAWK', 'uro', 'pornhub', 'panther', 'bookcorpus', 'cracking4crawling', 'crawly', 'Tor_Crawler', 'crawler', 'Crawling-Infrastructure']\n",
            "['Scrapy, a fast high-level web crawling & scraping framework for Python.', 'DotnetSpider, a .NET standard web crawling library. It is lightweight, efficient and fast high-level web crawling & sâ€¦', 'TuShare is a utility for crawling historical data of China stocks', 'A high-level distributed crawling framework.', 'Web crawling framework based on asyncio.', 'Crawljax: Crawling Dynamic (JavaScript-based) Web Applications', '\"å¥‡ä¼¢çˆ¬è™«\"æ˜¯åŸºäºsprint boot ã€ WebMagic å®ç° å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ã€æ–°é—»ã€csdnã€infoç­‰ç½‘ç«™æ–‡ç« çˆ¬å–ï¼Œå¯ä»¥åŠ¨æ€è®¾ç½®æ–‡ç« çˆ¬å–è§„åˆ™ã€æ¸…æ´—è§„åˆ™ï¼ŒåŸºæœ¬å®ç°äº†çˆ¬å–å¤§éƒ¨åˆ†ç½‘ç«™çš„æ–‡ç« ã€‚', 'Scrapy examples crawling Craigslist', 'Dungeon Crawl: Stone Soup official repository', 'scrapy examples for crawling zhihu and github', 'All in one tool for Information Gathering, Vulnerability Scanning and Crawling. A must have tool for all penetration â€¦', 'declutters url lists for crawling/pentesting', 'crawl webm and mp4', 'A browser testing and web crawling library for PHP and Symfony', 'Crawl BookCorpus', 'ä¸€äº›çˆ¬è™«ç›¸å…³çš„ç­¾åã€éªŒè¯ç ç ´è§£ï¼Œç›®å‰å·²æ¶‰åŠï¼šå°çº¢ä¹¦ã€‚', 'Crawly, a high-level web crawling & scraping framework for Elixir.', 'Web crawling with IP rotation via Tor', 'Scala DSL for web crawling', 'Distributed crawling infrastructure running on top of severless computation, cloud storage (such as S3) and sophisticâ€¦']\n",
            "['43.4k', '3.3k', '11.5k', '1.4k', '2k', '459', '294', '192', '1.7k', '217', '2k', '471', '521', '2.5k', '536', '81', '556', '196', '144', '321']\n",
            "['Python', 'C#', 'Python', 'Python', 'Python', 'JavaScript', 'Java', 'Python', 'C++', 'Python', 'PHP', 'Python', 'Python', 'PHP', 'Python', 'Python', 'Elixir', 'Python', 'Scala', 'TypeScript']\n",
            "['scrapy', 'dotnetcore', 'waditu', 'qinxuye', 'gaojiuli', 'crawljax', 'qiyaTech', 'mjhea0', 'crawl', 'chensoul', 'Tuhinshubhra', 's0md3v', 'hjkl01', 'symfony', 'soskek', 'crush-one', 'elixir-crawly', 'alex-miller-0', 'bplawler', 'NikolaiT', 'geziyor', 'meibenjin', 'ZhangBohan', 'ChrisRx', 'duckduckgo', 'kootenpv', 'zorlan', 'afunTW', 'codelucas', 'mion']\n",
            "['scrapy', 'DotnetSpider', 'tushare', 'cola', 'gain', 'crawljax', 'javaCrawling', 'Scrapy-Samples', 'crawl', 'scrapy-zhihu-github', 'RED_HAWK', 'uro', 'pornhub', 'panther', 'bookcorpus', 'cracking4crawling', 'crawly', 'Tor_Crawler', 'crawler', 'Crawling-Infrastructure', 'geziyor', 'GoogleSearchCrawler', 'fun_crawler', 'dungeonfs', 'duckduckcrawl', 'sky', 'skycaiji', 'Python-Crawling-Tutorial', 'newspaper', 'harvestman']\n",
            "['Scrapy, a fast high-level web crawling & scraping framework for Python.', 'DotnetSpider, a .NET standard web crawling library. It is lightweight, efficient and fast high-level web crawling & sâ€¦', 'TuShare is a utility for crawling historical data of China stocks', 'A high-level distributed crawling framework.', 'Web crawling framework based on asyncio.', 'Crawljax: Crawling Dynamic (JavaScript-based) Web Applications', '\"å¥‡ä¼¢çˆ¬è™«\"æ˜¯åŸºäºsprint boot ã€ WebMagic å®ç° å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ã€æ–°é—»ã€csdnã€infoç­‰ç½‘ç«™æ–‡ç« çˆ¬å–ï¼Œå¯ä»¥åŠ¨æ€è®¾ç½®æ–‡ç« çˆ¬å–è§„åˆ™ã€æ¸…æ´—è§„åˆ™ï¼ŒåŸºæœ¬å®ç°äº†çˆ¬å–å¤§éƒ¨åˆ†ç½‘ç«™çš„æ–‡ç« ã€‚', 'Scrapy examples crawling Craigslist', 'Dungeon Crawl: Stone Soup official repository', 'scrapy examples for crawling zhihu and github', 'All in one tool for Information Gathering, Vulnerability Scanning and Crawling. A must have tool for all penetration â€¦', 'declutters url lists for crawling/pentesting', 'crawl webm and mp4', 'A browser testing and web crawling library for PHP and Symfony', 'Crawl BookCorpus', 'ä¸€äº›çˆ¬è™«ç›¸å…³çš„ç­¾åã€éªŒè¯ç ç ´è§£ï¼Œç›®å‰å·²æ¶‰åŠï¼šå°çº¢ä¹¦ã€‚', 'Crawly, a high-level web crawling & scraping framework for Elixir.', 'Web crawling with IP rotation via Tor', 'Scala DSL for web crawling', 'Distributed crawling infrastructure running on top of severless computation, cloud storage (such as S3) and sophisticâ€¦', 'Geziyor, blazing fast web crawling & scraping framework for Go. Supports JS rendering.', 'a tool for crawl Google search results', 'Crawl some picture for fun', 'A FUSE filesystem and dungeon crawling adventure game engine', 'Distributed crawling prototype for DuckDuckGO', 'ğŸŒ… next generation web crawling using machine intelligence', 'è“å¤©é‡‡é›†å™¨æ˜¯ä¸€æ¬¾å…è´¹çš„æ•°æ®é‡‡é›†å‘å¸ƒçˆ¬è™«è½¯ä»¶ï¼Œé‡‡ç”¨php+mysqlå¼€å‘ï¼Œå¯éƒ¨ç½²åœ¨äº‘æœåŠ¡å™¨ï¼Œå‡ ä¹èƒ½é‡‡é›†æ‰€æœ‰ç±»å‹çš„ç½‘é¡µï¼Œæ— ç¼å¯¹æ¥å„ç±»CMSå»ºç«™ç¨‹åºï¼Œå…ç™»å½•å®æ—¶å‘å¸ƒæ•°æ®ï¼Œå…¨è‡ªåŠ¨æ— éœ€äººå·¥å¹²é¢„ï¼æ˜¯ç½‘é¡µå¤§æ•°æ®é‡‡é›†è½¯ä»¶ä¸­å®Œå…¨è·¨å¹³å°çš„äº‘ç«¯çˆ¬è™«ç³»ç»Ÿ', 'Python crawling tutorial', 'News, full-text, and article metadata extraction in Python 3. Advanced docs:', 'Quick and dirty web crawling.']\n",
            "['43.4k', '3.3k', '11.5k', '1.4k', '2k', '459', '294', '192', '1.7k', '217', '2k', '471', '521', '2.5k', '536', '81', '556', '196', '144', '321', '1.7k', '350', '166', '990', '142', '298', '1.5k', '61', '11.8k', '276']\n",
            "['Python', 'C#', 'Python', 'Python', 'Python', 'JavaScript', 'Java', 'Python', 'C++', 'Python', 'PHP', 'Python', 'Python', 'PHP', 'Python', 'Python', 'Elixir', 'Python', 'Scala', 'TypeScript', 'Go', 'Python', 'Python', 'Go', 'Python', 'Python', 'PHP', 'Jupyter Notebook', 'Python', 'Ruby']\n",
            "['scrapy', 'dotnetcore', 'waditu', 'qinxuye', 'gaojiuli', 'crawljax', 'qiyaTech', 'mjhea0', 'crawl', 'chensoul', 'Tuhinshubhra', 's0md3v', 'hjkl01', 'symfony', 'soskek', 'crush-one', 'elixir-crawly', 'alex-miller-0', 'bplawler', 'NikolaiT', 'geziyor', 'meibenjin', 'ZhangBohan', 'ChrisRx', 'duckduckgo', 'kootenpv', 'zorlan', 'afunTW', 'codelucas', 'mion', 'commoncrawl', 'clemfromspace', 'lgraubner', 'riywo', 'yatima1460', 'RoyalIcing', 'github', 'REMitchell', 'vanangamudi', 'apache']\n",
            "['scrapy', 'DotnetSpider', 'tushare', 'cola', 'gain', 'crawljax', 'javaCrawling', 'Scrapy-Samples', 'crawl', 'scrapy-zhihu-github', 'RED_HAWK', 'uro', 'pornhub', 'panther', 'bookcorpus', 'cracking4crawling', 'crawly', 'Tor_Crawler', 'crawler', 'Crawling-Infrastructure', 'geziyor', 'GoogleSearchCrawler', 'fun_crawler', 'dungeonfs', 'duckduckcrawl', 'sky', 'skycaiji', 'Python-Crawling-Tutorial', 'newspaper', 'harvestman', 'commoncrawl', 'scrapy-selenium', 'sitemap-generator-cli', 'law.e-gov.go.jp', 'Drill', 'Lantern', 'lightcrawler', 'python-crawling', 'newspaper-crawler-scripts', 'nutch']\n",
            "['Scrapy, a fast high-level web crawling & scraping framework for Python.', 'DotnetSpider, a .NET standard web crawling library. It is lightweight, efficient and fast high-level web crawling & sâ€¦', 'TuShare is a utility for crawling historical data of China stocks', 'A high-level distributed crawling framework.', 'Web crawling framework based on asyncio.', 'Crawljax: Crawling Dynamic (JavaScript-based) Web Applications', '\"å¥‡ä¼¢çˆ¬è™«\"æ˜¯åŸºäºsprint boot ã€ WebMagic å®ç° å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ã€æ–°é—»ã€csdnã€infoç­‰ç½‘ç«™æ–‡ç« çˆ¬å–ï¼Œå¯ä»¥åŠ¨æ€è®¾ç½®æ–‡ç« çˆ¬å–è§„åˆ™ã€æ¸…æ´—è§„åˆ™ï¼ŒåŸºæœ¬å®ç°äº†çˆ¬å–å¤§éƒ¨åˆ†ç½‘ç«™çš„æ–‡ç« ã€‚', 'Scrapy examples crawling Craigslist', 'Dungeon Crawl: Stone Soup official repository', 'scrapy examples for crawling zhihu and github', 'All in one tool for Information Gathering, Vulnerability Scanning and Crawling. A must have tool for all penetration â€¦', 'declutters url lists for crawling/pentesting', 'crawl webm and mp4', 'A browser testing and web crawling library for PHP and Symfony', 'Crawl BookCorpus', 'ä¸€äº›çˆ¬è™«ç›¸å…³çš„ç­¾åã€éªŒè¯ç ç ´è§£ï¼Œç›®å‰å·²æ¶‰åŠï¼šå°çº¢ä¹¦ã€‚', 'Crawly, a high-level web crawling & scraping framework for Elixir.', 'Web crawling with IP rotation via Tor', 'Scala DSL for web crawling', 'Distributed crawling infrastructure running on top of severless computation, cloud storage (such as S3) and sophisticâ€¦', 'Geziyor, blazing fast web crawling & scraping framework for Go. Supports JS rendering.', 'a tool for crawl Google search results', 'Crawl some picture for fun', 'A FUSE filesystem and dungeon crawling adventure game engine', 'Distributed crawling prototype for DuckDuckGO', 'ğŸŒ… next generation web crawling using machine intelligence', 'è“å¤©é‡‡é›†å™¨æ˜¯ä¸€æ¬¾å…è´¹çš„æ•°æ®é‡‡é›†å‘å¸ƒçˆ¬è™«è½¯ä»¶ï¼Œé‡‡ç”¨php+mysqlå¼€å‘ï¼Œå¯éƒ¨ç½²åœ¨äº‘æœåŠ¡å™¨ï¼Œå‡ ä¹èƒ½é‡‡é›†æ‰€æœ‰ç±»å‹çš„ç½‘é¡µï¼Œæ— ç¼å¯¹æ¥å„ç±»CMSå»ºç«™ç¨‹åºï¼Œå…ç™»å½•å®æ—¶å‘å¸ƒæ•°æ®ï¼Œå…¨è‡ªåŠ¨æ— éœ€äººå·¥å¹²é¢„ï¼æ˜¯ç½‘é¡µå¤§æ•°æ®é‡‡é›†è½¯ä»¶ä¸­å®Œå…¨è·¨å¹³å°çš„äº‘ç«¯çˆ¬è™«ç³»ç»Ÿ', 'Python crawling tutorial', 'News, full-text, and article metadata extraction in Python 3. Advanced docs:', 'Quick and dirty web crawling.', 'Common Crawl support library to access 2008-2012 crawl archives (ARC files)', 'Scrapy middleware to handle javascript pages using selenium', 'Creates an XML-Sitemap by crawling a given site.', 'Crawling Japanese laws', 'Search files without indexing, but clever crawling', 'Mac app for website auditing and crawling', 'Crawl a website and run it through Google lighthouse', 'Code Repository for Web Crawling with Python', 'set of scripts for crawling newspaper websites.', 'Apache Nutch is an extensible and scalable web crawler']\n",
            "['43.4k', '3.3k', '11.5k', '1.4k', '2k', '459', '294', '192', '1.7k', '217', '2k', '471', '521', '2.5k', '536', '81', '556', '196', '144', '321', '1.7k', '350', '166', '990', '142', '298', '1.5k', '61', '11.8k', '276', '470', '719', '242', '164', '245', '187', '1.4k', '29', '24', '2.4k']\n",
            "['Python', 'C#', 'Python', 'Python', 'Python', 'JavaScript', 'Java', 'Python', 'C++', 'Python', 'PHP', 'Python', 'Python', 'PHP', 'Python', 'Python', 'Elixir', 'Python', 'Scala', 'TypeScript', 'Go', 'Python', 'Python', 'Go', 'Python', 'Python', 'PHP', 'Jupyter Notebook', 'Python', 'Ruby', 'C++', 'Python', 'JavaScript', 'Ruby', 'D', 'Swift', 'JavaScript', 'JavaScript', 'Python', 'Java']\n",
            "['scrapy', 'dotnetcore', 'waditu', 'qinxuye', 'gaojiuli', 'crawljax', 'qiyaTech', 'mjhea0', 'crawl', 'chensoul', 'Tuhinshubhra', 's0md3v', 'hjkl01', 'symfony', 'soskek', 'crush-one', 'elixir-crawly', 'alex-miller-0', 'bplawler', 'NikolaiT', 'geziyor', 'meibenjin', 'ZhangBohan', 'ChrisRx', 'duckduckgo', 'kootenpv', 'zorlan', 'afunTW', 'codelucas', 'mion', 'commoncrawl', 'clemfromspace', 'lgraubner', 'riywo', 'yatima1460', 'RoyalIcing', 'github', 'REMitchell', 'vanangamudi', 'apache', 'apache', 'YahooArchive', 'CrawlScript', 'zhuyingda', 'zx576', 'FLZ101', 'trivio', 'evanzd', 'TeamHG-Memex', 'wukenaihe']\n",
            "['scrapy', 'DotnetSpider', 'tushare', 'cola', 'gain', 'crawljax', 'javaCrawling', 'Scrapy-Samples', 'crawl', 'scrapy-zhihu-github', 'RED_HAWK', 'uro', 'pornhub', 'panther', 'bookcorpus', 'cracking4crawling', 'crawly', 'Tor_Crawler', 'crawler', 'Crawling-Infrastructure', 'geziyor', 'GoogleSearchCrawler', 'fun_crawler', 'dungeonfs', 'duckduckcrawl', 'sky', 'skycaiji', 'Python-Crawling-Tutorial', 'newspaper', 'harvestman', 'commoncrawl', 'scrapy-selenium', 'sitemap-generator-cli', 'law.e-gov.go.jp', 'Drill', 'Lantern', 'lightcrawler', 'python-crawling', 'newspaper-crawler-scripts', 'nutch', 'nutch', 'anthelion', 'WebCollector', 'webster', 'recruitment', 'dl_coursera', 'common_crawl_index', 'ICLR2021-OpenReviewData', 'arachnado', 'db-meta']\n",
            "['Scrapy, a fast high-level web crawling & scraping framework for Python.', 'DotnetSpider, a .NET standard web crawling library. It is lightweight, efficient and fast high-level web crawling & sâ€¦', 'TuShare is a utility for crawling historical data of China stocks', 'A high-level distributed crawling framework.', 'Web crawling framework based on asyncio.', 'Crawljax: Crawling Dynamic (JavaScript-based) Web Applications', '\"å¥‡ä¼¢çˆ¬è™«\"æ˜¯åŸºäºsprint boot ã€ WebMagic å®ç° å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ã€æ–°é—»ã€csdnã€infoç­‰ç½‘ç«™æ–‡ç« çˆ¬å–ï¼Œå¯ä»¥åŠ¨æ€è®¾ç½®æ–‡ç« çˆ¬å–è§„åˆ™ã€æ¸…æ´—è§„åˆ™ï¼ŒåŸºæœ¬å®ç°äº†çˆ¬å–å¤§éƒ¨åˆ†ç½‘ç«™çš„æ–‡ç« ã€‚', 'Scrapy examples crawling Craigslist', 'Dungeon Crawl: Stone Soup official repository', 'scrapy examples for crawling zhihu and github', 'All in one tool for Information Gathering, Vulnerability Scanning and Crawling. A must have tool for all penetration â€¦', 'declutters url lists for crawling/pentesting', 'crawl webm and mp4', 'A browser testing and web crawling library for PHP and Symfony', 'Crawl BookCorpus', 'ä¸€äº›çˆ¬è™«ç›¸å…³çš„ç­¾åã€éªŒè¯ç ç ´è§£ï¼Œç›®å‰å·²æ¶‰åŠï¼šå°çº¢ä¹¦ã€‚', 'Crawly, a high-level web crawling & scraping framework for Elixir.', 'Web crawling with IP rotation via Tor', 'Scala DSL for web crawling', 'Distributed crawling infrastructure running on top of severless computation, cloud storage (such as S3) and sophisticâ€¦', 'Geziyor, blazing fast web crawling & scraping framework for Go. Supports JS rendering.', 'a tool for crawl Google search results', 'Crawl some picture for fun', 'A FUSE filesystem and dungeon crawling adventure game engine', 'Distributed crawling prototype for DuckDuckGO', 'ğŸŒ… next generation web crawling using machine intelligence', 'è“å¤©é‡‡é›†å™¨æ˜¯ä¸€æ¬¾å…è´¹çš„æ•°æ®é‡‡é›†å‘å¸ƒçˆ¬è™«è½¯ä»¶ï¼Œé‡‡ç”¨php+mysqlå¼€å‘ï¼Œå¯éƒ¨ç½²åœ¨äº‘æœåŠ¡å™¨ï¼Œå‡ ä¹èƒ½é‡‡é›†æ‰€æœ‰ç±»å‹çš„ç½‘é¡µï¼Œæ— ç¼å¯¹æ¥å„ç±»CMSå»ºç«™ç¨‹åºï¼Œå…ç™»å½•å®æ—¶å‘å¸ƒæ•°æ®ï¼Œå…¨è‡ªåŠ¨æ— éœ€äººå·¥å¹²é¢„ï¼æ˜¯ç½‘é¡µå¤§æ•°æ®é‡‡é›†è½¯ä»¶ä¸­å®Œå…¨è·¨å¹³å°çš„äº‘ç«¯çˆ¬è™«ç³»ç»Ÿ', 'Python crawling tutorial', 'News, full-text, and article metadata extraction in Python 3. Advanced docs:', 'Quick and dirty web crawling.', 'Common Crawl support library to access 2008-2012 crawl archives (ARC files)', 'Scrapy middleware to handle javascript pages using selenium', 'Creates an XML-Sitemap by crawling a given site.', 'Crawling Japanese laws', 'Search files without indexing, but clever crawling', 'Mac app for website auditing and crawling', 'Crawl a website and run it through Google lighthouse', 'Code Repository for Web Crawling with Python', 'set of scripts for crawling newspaper websites.', 'Apache Nutch is an extensible and scalable web crawler', 'Apache Nutch is an extensible and scalable web crawler', 'Anthelion is a plugin for Apache Nutch to crawl semantic annotations within HTML pages.', 'WebCollector is an open source web crawler framework based on Java.It provides some simple interfaces for crawling thâ€¦', 'a reliable high-level web crawling & scraping framework for Node.js.', 'A project crawling online recruitment websites getting offer information', 'A simple, fast, and reliable Coursera crawling & downloading tool', 'Index URLs in Common Crawl', 'Crawl & visualize ICLR papers and reviews.', 'Web Crawling UI and HTTP API, based on Scrapy and Tornado', 'database meta crawl tool']\n",
            "['43.4k', '3.3k', '11.5k', '1.4k', '2k', '459', '294', '192', '1.7k', '217', '2k', '471', '521', '2.5k', '536', '81', '556', '196', '144', '321', '1.7k', '350', '166', '990', '142', '298', '1.5k', '61', '11.8k', '276', '470', '719', '242', '164', '245', '187', '1.4k', '29', '24', '2.4k', '2.4k', '2.9k', '2.9k', '385', '63', '128', '174', '408', '148', '78']\n",
            "['Python', 'C#', 'Python', 'Python', 'Python', 'JavaScript', 'Java', 'Python', 'C++', 'Python', 'PHP', 'Python', 'Python', 'PHP', 'Python', 'Python', 'Elixir', 'Python', 'Scala', 'TypeScript', 'Go', 'Python', 'Python', 'Go', 'Python', 'Python', 'PHP', 'Jupyter Notebook', 'Python', 'Ruby', 'C++', 'Python', 'JavaScript', 'Ruby', 'D', 'Swift', 'JavaScript', 'JavaScript', 'Python', 'Java', 'Java', 'Java', 'Java', 'JavaScript', 'JavaScript', 'Python', 'Python', 'Jupyter Notebook', 'Python', 'Java']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "users = []\n",
        "repositories = []\n",
        "descriptions = []\n",
        "stars = []\n",
        "languages = []\n",
        "\n",
        "# csv ì œëª© ì§€ì •\n",
        "f = open(\"data.csv\", \"w\", encoding='utf-8')\n",
        "writer = csv.writer(f)\n",
        "data = [['user', 'repository', 'descripsion', 'star', 'language']]\n",
        "writer.writerows(data)\n",
        "\n",
        "\n",
        "# ë°ì´í„°ë¥¼ ìœ„ users, repositories, descriptions, stars, languages ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•˜ëŠ”í•¨ìˆ˜\n",
        "def get_data(url):\n",
        "  \n",
        "    # url = 'https://github.com/search?p=1&q=crawling&type=Repositories'\n",
        "    response = requests.get(url)\n",
        "    title_all = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        html = response.text\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        # ì œëª©ì˜ classë¥¼ ì´ìš©í•´ ì œëª©ë°›ì•„ì˜´.\n",
        "        attrs = {'class': 'v-align-middle'}\n",
        "        title = soup.find_all('a', attrs=attrs)\n",
        "        # print(title)\n",
        "        for i in title:\n",
        "            # ì œëª©ì€ scrapy/scrapy í˜•ì‹ì´ê³ , ë‚˜ì¤‘ì— ì´ë¥¼ ì“¸ì¼ì´ ìˆê¸°ì— title_allì— ì €ì¥, ê·¸ë¦¬ê³  '/'ë¡œ ìŠ¤í”Œë¦¿í•˜ì—¬ ìœ ì €ëª…ê³¼ ë¦¬í¬ì§€í† ë¦¬ëª… ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
        "            title_all.append(i.get_text())\n",
        "            titles = i.get_text().split('/')\n",
        "            users.append(titles[0])\n",
        "            repositories.append(titles[1])\n",
        "        print(users)\n",
        "        print(repositories)\n",
        "\n",
        "\n",
        "        # ì„¤ëª…ë¶€ë¶„ì˜ classë¥¼ ì´ìš©í•´ ì„¤ëª… ë°›ì•„ì˜´. í…ìŠ¤íŠ¸ë¥¼ ë½‘ìœ¼ë©´ \\nê³¼ ê³µë°±ë“¤ì´ ë§ì•„ì„œ strip()ìœ¼ë¡œ ì•ë’¤ ë¶ˆí•„ìš”ìš”ì†Œ ì œê±°í•¨.(ì²˜ìŒì—” ê³µë°±ê³¼ \\nì˜ ê°œìˆ˜ê°€ ì¼ì •í•´ì„œ ì¸ë±ìŠ¤ë¡œ ê°€ì ¸ì™”ì—ˆìŒ.)\n",
        "        attrs = {'class': 'mb-1'}\n",
        "        description = soup.find_all('p', attrs=attrs)\n",
        "        # print(description)\n",
        "        for i in description:\n",
        "            # descriptions.append(i.get_text()[9:-7])\n",
        "            descriptions.append(i.get_text().strip())\n",
        "        print(descriptions)\n",
        "\n",
        "\n",
        "        # ë³„ ê°¯ìˆ˜ ê°€ì ¸ì˜¤ëŠ”ë° classê°€ Link--mutedì¸ê±¸ ê°€ì ¸ì™€ë³´ë‹ˆ classê°€ Link--muted f6ì¸ '30 issues need help' ê°™ì€ ë¬¸ìë„ ê°™ì´ ì˜´\n",
        "        # ê·¸ë˜ì„œ classì™€ href ë‘˜ë‹¤ ë§Œì¡±í•˜ëŠ” ì¡°ê±´ìœ¼ë¡œ starê°€ì ¸ì˜´. hrefê°€ /scrapy/scrapy/stargazers ë¡œ '/' + ì œëª©ì „ì²´ê°’ + '/stargazers'ì´ê¸°ì— ì•„ê¹Œ title_allì„ ë”°ë¡œ ì €ì¥í–ˆë‹¤ ê°€ì ¸ì˜´\n",
        "        # print(title_all)\n",
        "        for i in range(len(title_all)):\n",
        "            href = '/' + title_all[i] + '/stargazers'\n",
        "            # print(href)\n",
        "            attrs = {'class': 'Link--muted', 'href': href}\n",
        "            star = soup.find('a', attrs=attrs)\n",
        "            # stars.append(star.get_text()[18:-13])\n",
        "            stars.append(star.get_text().strip())\n",
        "        print(stars)\n",
        "\n",
        "        # í”„ë¡œê·¸ë˜ë°ì–¸ì–´ëŠ” itemprop ê°’ì´ programmingLanguageì¸ê±¸ë¡œ ì°¾ì•„ì˜´.\n",
        "        attrs = {'itemprop': 'programmingLanguage'}\n",
        "        language = soup.find_all('span', attrs=attrs)\n",
        "        # print(language)\n",
        "        for i in language:\n",
        "            languages.append(i.get_text())\n",
        "        print(languages)\n",
        "\n",
        "    else:\n",
        "        print('ì—°ê²°ë˜ì§€ì•ŠìŒ')\n",
        "        print(response.status_code)\n",
        "\n",
        "\n",
        "# í‚¤ì›Œë“œì…ë ¥\n",
        "keyword = input()\n",
        "\n",
        "# 5í˜ì´ì§€ê¹Œì§€ í‚¤ì›Œë“œë¥¼ ë„£ì–´ì„œ ê²€ìƒ‰í•˜ì—¬ users, repositories, descriptions, stars, languages ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
        "for i in range(1, 6):\n",
        "    url = 'https://github.com/search?p='+str(i)+'&q='+keyword+'&type=Repositories'\n",
        "    # ìœ„ì— ì •ì˜í•œ ë¦¬ìŠ¤íŠ¸ì— ìš”ì†Œë“¤ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜(1~5í˜ì´ì§€ê¹Œì§€ ëŒë©´ì„œ appendí•¨)\n",
        "    get_data(url)\n",
        "    # ë¹ ë¥¸ì ‘ê·¼ì‹œ ë¬¸ì œë˜ì–´ 10ì´ˆê°„ ì—¬ìœ  ë‘ . ìµœì ì‹œê°„ì„ ì•Œë©´ ì¤„ì¼ ìˆ˜ ìˆì„ë“¯.\n",
        "    time.sleep(10)\n",
        "\n",
        "# csvì— ìœ„ ì„¤ì •í•œ ì œëª©ì— ë§ê²Œë” ì¤„ë³„ë¡œ ì…ë ¥\n",
        "for i in range(len(repositories)):\n",
        "    row = [[users[i], repositories[i], descriptions[i], stars[i], languages[i]]]\n",
        "    writer.writerows(row)\n",
        "\n",
        "# ë‹¤ ì…ë ¥í•˜ê³  íŒŒì¼ ë‹«ìŒ\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BeautifulSoup.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
