from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
driver = webdriver.Chrome(executable_path= "./chromedriver.exe")
url = ""
driver.get(url)
time.sleep(10)
serch=driver.find_element_by_xpath('')
serch.click()
driver.find_element_by_xpath('').send_keys('')
serch.send_keys(Keys.ENTER)
res = []
for _ in range(5):
time.sleep(10)
for j in range(1,11):




#수정수정수정

try:
# 먼저 레포지토리의 제작자와 레포지토리의 이름을 가져오는 변수입니다.
name = driver.find_element_by_xpath(f'//*[@id="js-pjax-container"]/div/div[3]/div/ul/li[{j}]/div[2]/div[1]/div/a').text
# 잘 되고 있는지 확인 차 print 문으로 해당 변수와 경계를 표시해줍니다.
print(f'---{name}---')
except:
# 타겟에 안 맞으면 변수에 결측치 표시를 위해 NAN을 할당해주고 다음 행동으로 넘어갑니다.
name = "NAN"
pass
try:
# name 처럼 추천을 받은 수도 해줍니다. 아래도 마찬가지로 사용된 언어, 업데이트된 날짜도 같은 방식으로 해줍니다.
stars = driver.find_element_by_xpath(f'//*[@id="js-pjax-container"]/div/div[3]/div/ul/li[{j}]/div[2]/div[2]/div/div[1]/a').text
print(f'---{stars}---')
except:
stars = "NAN"
pass
try:
lang = driver.find_element_by_xpath(f'//*[@id="js-pjax-container"]/div/div[3]/div/ul/li[{j}]/div[2]/div[2]/div/div[2]/span/span[2]
print(f'---{lang}---')
except :
lang = "NAN"
pass
try:
date = driver.find_element_by_xpath(f'//*[@id="js-pjax-container"]/div/div[3]/div/ul/li[{j}]/div[2]/div[2]/div/div[3]').text
print(f'---{date}---')
except:
date = "NAN"
pass
# 한 목록의 크롤링이 끝나면 res 변수에 원소를 추가 해주고 다음 목록으로 넘어갑니다.
res.append((name,stars,lang,date))

driver.find_element_by_css_selector('#js-pjax-container > div > div.col-12.col-md-9.float-left.px-2.pt-3.pt-md-0.codesearch-results >

print(res)
driver.quit()
import pandas as pd
# 우리가 크롤링한 데이터를 데이터 프레임 변수에 할당해줍니다.
data = pd.DataFrame(res)
# 그리고 우리가 작업하고 있는 디렉토리에 csv파일을 저장해줍니다.
data.to_csv('./data.csv')
